## 1.机器学习概述
### 思维引导&&机器学习相关名词解释
![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1761902978278-d2d4f637-ae53-4968-b301-bc50f1259ead.png)

#### 机器学习
![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1761988530665-d2621a97-7b77-4559-8b38-33f9c20d4a19.png)

+ 机器学习分类方式：
    - 学习方式	根据徐连数据是否包含标签进行分类：
        * 监督学习
            + 使用带标签的训练数据，模型学习从输入到输出的映射关系
        * 无监督学习
            + 处理没有标签的数据，发现数据中潜在的结构和模式
        * 半监督学习
            + 同时使用有标签和无标签的数据进行训练，以提高模型性能
            + 相对有监督而言的学习成本低，准确率高
        * 强化学习
            + 通过智能体与环境的交互，根据获得的奖励来学习最优策略
    - 学习任务	根据目标输出的特点进行分类：
        * 分类	将输入数据分配到预定义的类别中，例如识别垃圾邮件
            + 二分类	预测两个类别之一
            + 多分类	从两个以上的类别中选择一个
            + 多标签分类	为一个样本分配一个或多个标签
        * 回归	预测一个连续的数值输出
        * 聚类	将相似的数据点分组到一起
        * 降维	减少数据的特征数量，同时保留重要信息
+ 所有类型的机器学习都有以下两个步骤：
    - 学习步骤
        * 通过归纳分析训练样本集来建立分类模型得到分类规则
    - 预测步骤
        * 先用已知的测试样本集评估分类规则的准确率，若准确率可接受，则使用该模型对未知类标签的待测样本集进行预测
    - 一般将数据按7/3分为训练数据和待测数据  随着数据数量的提高前者比重越来越大
    - 或分为训练数据，验证数据和待测数据	建模，调整，验证	3：1：1
+ 机器学习应用领域：
    - 数据分析
    - 计算机视觉
    - 自然语言处理
    - 生物特征识别
    - 搜索引擎
    - 医学诊断
    - 检测信用卡欺诈
    - 证券市场分析
    - DNA序列测序
    - 语音与手写识别
    - 战略游戏
    - 机器人
    - ......

![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1761994086898-33a3dc77-f4ad-4f9f-9738-36fd57a1bb56.png)

---

## 2.基础算法
### 分类算法
+ 一种对离散型随机变量建模或预测的监督学习方法
+ 其实是求取一个从输入变量（特征）到离散的输出变量（标签）之间的映射函数
+ 分类算法基本可以分成两类：
    - 基本分类器：
        * 逻辑回归
        * KNN
        * 朴素贝叶斯
        * SVM
        * 决策树
        * ......
    - 集成分类器：
        * Boosting
        * Bagging
        * Stacking
        * ......

---

#### 逻辑回归
+ 即概率型非线性回归，能够实现二分类与多分类
+ 数据输入要求：
    - 特征为数据性，标签为概率值0或1，多分类情况需要对标签进行独热编码处理
    - 数据中不能存在空值
    - 特征与标签之间存在线性相关性
    - 特征需要进行标准化处理
    - 较为平衡的正负样本比例

![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1762072471072-e7d28582-a638-4fd3-b0fc-05ee35312c93.png)

![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1762072562710-602a0dc8-7461-4dbb-a4e3-5a101bc67ff5.png)

---

实践：

+ 可打开anaconda的jupyter

```python
# 导入必要的库
import numpy as np  # 用于数值计算的基础库
import pandas as pd  # 用于数据处理和分析的库（核心是DataFrame数据结构）
from sklearn.linear_model import LogisticRegression  # 导入逻辑回归模型（用于分类任务）
from sklearn.model_selection import train_test_split  # 导入数据集拆分工具（拆分训练集和测试集）

# 读取CSV数据文件
# "D:\\credit.csv"是文件路径（注意Windows系统中路径用双反斜杠转义）
# index_col=0表示将数据的第1列作为DataFrame的索引（而非默认的0,1,2...）
data = pd.read_csv("D:\\credit.csv", index_col=0)

# 显示数据的前5行（默认），用于快速查看数据结构（列名、数据格式等），确认数据是否正确读取
data.head()

# 将数据集拆分为训练集（train）和测试集（test）
# test_size=0.2表示测试集占总数据的20%，训练集占80%（拆分是随机的）
train, test = train_test_split(data, test_size=0.2)

# 定义目标变量（要预测的列）的名称，这里是"Target"（例如：信用违约标签0/1）
y_cols = ["Target"]

# 定义特征变量（用于预测的列）的名称：所有列中排除目标变量"Target"的列
x_cols = [x for x in data.columns if x not in y_cols]

# 从训练集中提取特征变量（输入），用于模型训练
train_x = train[x_cols]

# 从训练集中提取目标变量（输出标签），用于模型训练
train_y = train[y_cols]

# 实例化一个逻辑回归模型（使用默认参数，如 solver='lbfgs' 等）
model = LogisticRegression()

# 用训练集的特征和标签训练模型（核心步骤：模型通过数据学习参数）
# fit方法会根据train_x（输入）和train_y（输出）调整模型参数，得到训练好的模型
Target_model = model.fit(train_x, train_y)

# 从测试集中提取特征变量（输入），用于模型预测
test_x = test[x_cols]

# 从测试集中提取目标变量（真实标签），用于后续评估预测结果（代码中未体现评估步骤）
test_y = test[y_cols]

# 用训练好的模型对测试集的特征进行预测，得到预测的目标变量值（如0/1）
pred_y = model.predict(test_x)

# 输出测试集的预测结果（展示预测标签）
pred_y
```

+ 实现了一个完整的逻辑运算回归分类模型的训练与预测流程
+ 核心目的是通过历史数据训练模型，然后使用模型预测新数据的目标标签
+ 具体流程：
    - 环境准备
        * 
    - 数据读取
        * 

---

#### KNN
+ 假设给定一个训练数据集，其中实例的类别已定。分类时，对新的实例根据其K个最近相邻的训练实例的类别，通过多数表决等方式进行预测，因此不具有显性的学习过程
+ 利用训练数据集对特征向量空间进行划分，并作为其分类的模型
+ 基本要素：
    - K值的选择
    - 距离度量
    - 分类决策规则
+ 数据输入要求：
    - 特征为数据型，标签为类别
    - 数据中不能存在空值
    - 如果使用常规的欧式距离计算，需要对特征进行标准化处理
    - 较为平衡的正负样本比例

![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1762181912136-09e48441-b3ab-4265-ab25-01d7574c3990.png)

![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1762181972565-31c07c5c-0c06-4aaa-959b-9b6685be5f4d.png)

---

```python
# 从scikit-learn库的neighbors模块导入K近邻分类器（KNeighborsClassifier）
from sklearn.neighbors import KNeighborsClassifier	#sklearn Python机器学习常用库scikit-learn的简写

# 导入numpy库（用于数值计算的基础库），并简写为np（约定俗成的简写方式）
import numpy as np

# 定义特征数据集X：二维numpy数组，每行代表一个样本，每列代表一个特征
# 这里包含6个样本，每个样本有2个特征（例如可理解为"长度"和"宽度"等特征）
X = np.array([[1,1],[1,1.5],[2,2.5],[2.5,3],[1.5,1],[3,2.5]])

# 定义标签数据集Y：与X中样本一一对应，每个元素是样本的类别标签（这里为'A'或'B'）
# 即X中第1个样本标签为'A'，第2个为'A'，第3个为'B'，以此类推
Y = ['A','A','B','B','A','B']

# 实例化K近邻分类器模型
# 参数说明：
# - n_neighbors = 4：指定"近邻数量"为4，即预测时参考最近的4个样本的标签
# - algorithm = 'ball_tree'：指定计算近邻的算法为ball_tree（适用于高维数据，效率较高）
model = KNeighborsClassifier(n_neighbors = 4,algorithm = 'ball_tree')

# 用训练数据拟合（训练）模型
# 注：原代码"x,y"为笔误，应与定义的变量保持一致，即"X,Y"
# 作用：让模型通过学习X（特征）和Y（标签）的对应关系，建立分类规则
model.fit(X,Y)

# 预测新样本的类别
# 输入：待预测的样本[[1.75,1.75]]（一个具有2个特征的新样本）
# 输出：该样本被预测的类别标签（基于最近的4个训练样本的标签多数投票结果）
print(model.predict([[1.75,1.75]]))

# 预测新样本属于各个类别的概率
# 输出：该样本属于每个类别的概率值（总和为1，例如可能返回[A的概率, B的概率]）
print(model.predict_proba([[1.75,1.75]]))

# 计算模型在训练数据上的准确率
# 注：原代码"x,y"为笔误，应改为"X,Y"
# 作用：评估模型对训练样本的分类效果，返回正确分类的样本数占总样本数的比例（0~1之间）
print(model.score(X,Y))
```

+ 实现了一个简单的K近邻分类任务（KNN）
    - 准备带标签的训练数据
    - 定义KNN模型（指定近邻数量为4，计算算法为ball_tree）
    - 用训练数据训练模型
    - 对新样本进行类别预测、概率预测，并评估模型在训练数据上面的准确性

---

#### 朴素贝叶斯
+ 应用最广泛的分类算法之一
+ 朴素贝叶斯算法是在贝叶斯算法的基础上进行了相应的简化，即假定给定目标值时特征之间相互条件独立
+ 对于决策结果，没有哪个特征占着较大或较小的比重

---

#### SVM
+ 支持向量机算法
+ 支持向量机的基本模型是定义在特征空间间隔最大的线性分类器，包括核函数，使得它成为实质上的非线性分类器
+ 支持向量机的学习策略是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题
+ 支持向量机的学习算法是求解凸二次规划的最优化方法
+ 支持向量机能够对非线性决策边界进行建模，又有许多可选择的核函数
+ 在面对过拟合时，有极强的稳健性，尤其在高维空间中
+ 但是支持向量机是内存密集型算法，选择正确的核函数需要相当的技巧，不宜用于较大的数据集
+ 算法数据输入要求：
    - 特征类型为数据型
    - 特征不允许缺失值
    - 使用常规线性核时需要进行标准化
    - 较为平衡的正负样本数

![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1762247315896-9156c1dd-a967-4a5c-b42b-fd19806827bb.png)

---

#### 决策树
+ 树状结构，包含一个根节点，若干内部节点，若干叶节点
+ 根节点包含样本全集，叶节点对应决策结果，内部节点对应一个特征测试
+ 从根节点到每个叶结点的路径对应一个判定测试序列
+ 学习目的是为了产生一颗泛化能力强，或处理未见实例能力强的决策树，其基本流程遵循简单而直观的分而治之策略，决策树的生成是一个递归的过程
+ 算法数据输入要求：
    - 特征类型无要求
    - 可以允许缺失值
    - 每个特征同等重要
    - 较为平衡的正负样本比例

![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1762332994294-ba9bc190-cdb5-4fb6-b277-f8726b219af5.png)![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1762333019157-4f301809-7174-4809-b4c0-8e258f59f7b4.png)

---

#### 多层感知机
+ 简称：MLP，有感知机推广而来，克服感知机无法实现对线性不可分数据的识别的缺点
+ 一种前向结构的人工神经网络，映射一组输入变量到输出变量
+ 可以被看作是一个有向图，有多个节点成组成，每一层全连接到下一层。
+ 除输入节点，每个节点都是一个非线性激活函数的神经元（或称处理单元）
+ 一种称为反向传播算法的监督学习方法常被用于训练MLP
+ 主要特点：有多个神经元层，第一成为输入层，最后一层为输出层，中间层为隐层
+ 算法数据输入要求：
    - 特征要求为数据型，标签要求为类别
    - 不允许缺失值
    - 特征需要进行标准化或归一化
    - 较为平衡的正负样本比例

![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1762334754685-cbae3f77-cd66-4a51-b9d7-a7af9f5abdcb.png)

![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1762334766267-6c6c56ef-87b8-4d0b-9be0-9ae2aeaba778.png)

---

### 回归算法
+ 回归算法实现步骤：
    - 学习
        * 学习是通过训练数据来拟合回归方程
    - 预测
        * 预测是利用学习过程中拟合的回归方程，将测试数据放入方程中求出预测值

![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1762338142617-d7335aa6-3a8a-4d0a-a56d-1959d79dc95f.png)

---

#### 线性回归
+ 机器学习中最基础的算法之一，利用数理统计中的回归分析。来确定两种或两种以上变量之间相互依赖的定量关系的一种统计分析的方法
+ 目标是在给定自变量的情况下预测因变量的值
+ 根据不同自变量数量，可以将线性回归分为一元线性回归和多元线性回归
+ 一元线性回归数据输入要求：
    - 自变量可以是分类变量和连续变量，因变量必须是连续变量
    - 因变量与自变量之间是线性关系
    - 自变量个数只能是1

![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1762339689643-47e3b04b-1df2-4b55-b075-9b07abeab1d4.png)

+ 多元线性回归
    - 多元线性回归模型的目的是构建一个回归方程，利用多个自变量估计因变量，从而解释和预测因变量的值
    - 多元线性回归模型中的因变量和大部分自变量为定量值，某些定性指标需要转换为定量值才能应用到回归方程中
    - 数据输入要求：
        * 自变量可以是分类变量和连续变量，因变量必须是连续变量
        * 因变量与自变量之间是线性关系
        * 自变量个数不少于2个
        * 自变量之间相互独立

#### KNN回归
+ KNN算法不仅可以用来分类，还可以用来进行回归分析
+ 原理是从训练数据中找到与新数据点在距离上最近的预定数量的几个数据点，并从这些数据点中预测标签
+ 这些数据点的数量可以是用户自定义的常量，也可以根据不同点的局部密度得到。距离通常可以通过任何方式来度量，标准欧式距离是最常见的选择之一
+ 数据输入要求：
    - 目标数据为连续型
    - 数据中不能存在空值

![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1762346908907-53568deb-a526-4c1b-99d6-d2be3a61fbe8.png)

---

#### Lasso回归
+ 以缩小特征集（降价）为思想，是一种收缩估计方法
+ 可以将特征的系数进行压缩并使某些回归系数变成0，进而达到特征选择的目的，可以广泛地应用于模型改进与选择。通过选择惩罚函数，借用Lasso思想与方法实现特征选择的目的
+ 模型选择本质上是寻求没模型稀疏表达的过程，而这种过程可以通过优化一个“损失”+"惩罚"的函数问题来解决
+ 算法数据输入要求：
    - 在理论上，Lasso回归对数据类型没有太多限制，可以接受任何类型的数据，而且一般不需要对类型进行标准化处理

![](https://cdn.nlark.com/yuque/0/2025/png/59258918/1762498073904-591dccd5-b9f0-43d5-9251-763cbc6ee62b.png)



